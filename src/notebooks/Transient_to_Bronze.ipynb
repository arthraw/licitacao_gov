{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Pipeline: Transient to bronze layer\n",
        "\n",
        "Here our data will be \"copied\" to the bronze stage, but before we create a delta table to store data. And then we copy to bronze, maintaining the original data structure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import col, current_timestamp, input_file_name\n",
        "from delta.tables import DeltaTable\n",
        "from notebookutils import mssparkutils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "parameters"
        ]
      },
      "outputs": [],
      "source": [
        "# Parametro do arquivo de input\n",
        "input_file = \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if not input_file:\n",
        "    print(\"Aviso: Parâmetro input_file não recebido.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "domain = \"Licitacoes_Gov\"\n",
        "storage_account = \"lablicitacoessa\"\n",
        "\n",
        "transient_path = f\"abfss://transient@{storage_account}.dfs.core.windows.net/{domain}/\"\n",
        "bronze_path    = f\"abfss://bronze@{storage_account}.dfs.core.windows.net/{domain}/\"\n",
        "archive_path   = f\"abfss://archive@{storage_account}.dfs.core.windows.net/{domain}/\"\n",
        "\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(f\"Domínio: {domain}\")\n",
        "print(f\"Transient: {transient_path}\")\n",
        "print(f\"Bronze: {bronze_path}\")\n",
        "print(f\"Archive: {archive_path}\")\n",
        "print(f\"{'='*80}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_raw = (\n",
        "    spark.read\n",
        "    .format(\"json\")\n",
        "    .load(input_file)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_bronze = (\n",
        "    df_raw\n",
        "    .select(\n",
        "        col(\"_airbyte_raw_id\").alias(\"airbyte_id\"),\n",
        "        col(\"_airbyte_extracted_at\").cast(\"timestamp\").alias(\"data_extracao_airbyte\"),\n",
        "        col(\"_airbyte_data.*\")\n",
        "    )\n",
        "    .withColumn(\"data_processamento_bronze\", current_timestamp())\n",
        "    .withColumn(\"arquivo_origem\", input_file_name())\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "(\n",
        "    df_bronze.write\n",
        "    .format(\"delta\")\n",
        "    .mode(\"append\")\n",
        "    .partitionBy(\"data_extracao_airbyte\")\n",
        "    .save(bronze_path)\n",
        ")\n",
        "print(\"Dados gravados com sucesso na camada bronze\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    spark.sql(f\"\"\"\n",
        "CREATE TABLE IF NOT EXISTS bronze_{domain.lower()}\n",
        "USING DELTA\n",
        "LOCATION '{bronze_path}'\n",
        "    \"\"\")\n",
        "except Exception as e:\n",
        "    print(f\"\\n Erro ao criar tabela bronze_{domain.lower()}: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for file_path in jsonl_files:\n",
        "    file_name = file_path.split(\"/\")[-1]\n",
        "    fs.mv(\n",
        "        file_path,\n",
        "        f\"{archive_path}{file_name}\"\n",
        "    )\n",
        "\n",
        "print(\"Arquivos movidos para archive com sucesso\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "python",
      "name": "synapse_pyspark"
    },
    "language_info": {
      "name": "python"
    },
    "save_output": true,
    "synapse_widget": {
      "state": {},
      "version": "0.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
